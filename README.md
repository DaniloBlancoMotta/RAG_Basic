# RAG_Basic

# 

## ğŸš€ Funcionalidades

- ğŸ“š Aprendizado personalizado
- ğŸ¤– IA adaptativa
- ğŸ“Š AnÃ¡lise de desempenho
- ğŸ’¡ RecomendaÃ§Ãµes inteligentes

## ğŸ› ï¸ Tecnologias

- Python
- Jupyter Notebook
- RAG (Retrieval-Augmented Generation)
- Processamento de Linguagem Natural

## ğŸ“¦Como usar?

# Instale as dependÃªncias
pip install -r requirements.txt
```

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

# Importando as bibliotecas necessÃ¡rias
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel
import faiss
import torch
import warnings
warnings.filterwarnings("ignore")

# Base de conhecimento
documents = [
    """Danilo Ã© um Especialista em Machine Learning e GestÃ£o de Agentes de IA, com foco em automaÃ§Ã£o de atendimento, 
    anÃ¡lise preditiva e integraÃ§Ã£o de sistemas. Atuo no desenvolvimento de modelos supervisionados e nÃ£o supervisionados 
    (classificaÃ§Ã£o, churn, recomendaÃ§Ã£o), usando Python, Scikit-Learn, Pandas e TensorFlow. AlÃ©m de execuÃ§Ã£o e construÃ§Ã£o 
    projetos com RAG para criar agentes de IA com respostas contextuais, combinando modelos generativos com recuperaÃ§Ã£o 
    de dados em tempo real. PÃ³s-graduado em InteligÃªncia Artificial e Machine Learning (XP EducaÃ§Ã£o), com formaÃ§Ã£o 
    complementar em GestÃ£o de NegÃ³cios e CiÃªncias da SaÃºde. Trabalho na interseÃ§Ã£o entre tecnologia, dados e estratÃ©gia"""
]

# Inicializando os modelos
class RAGSystem:
    def __init__(self):
        # Modelo para embeddings
        self.embed_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
        self.embed_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
        
        # Modelo gerador
        self.gen_tokenizer = AutoTokenizer.from_pretrained("t5-small")
        self.gen_model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
        
        # Criando Ã­ndice FAISS
        self.document_embeddings = self.embed_documents(documents)
        self.index = self.create_faiss_index(self.document_embeddings)
    
    def embed_documents(self, texts):
        inputs = self.embed_tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            embeddings = self.embed_model(**inputs).last_hidden_state.mean(dim=1)
        return embeddings.numpy()
    
    def create_faiss_index(self, embeddings):
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings)
        return index
    
    def retrieve_documents(self, question, top_k=1):
        question_embedding = self.embed_documents([question])
        distances, indices = self.index.search(question_embedding, top_k)
        return [documents[idx] for idx in indices[0]]
    
    def generate_answer(self, context, question):
        input_text = f"contexto: {context} pergunta: {question}"
        inputs = self.gen_tokenizer.encode(input_text, return_tensors="pt", truncation=True)
        with torch.no_grad():
            outputs = self.gen_model.generate(inputs, max_length=150, num_beams=4, temperature=0.7)
        return self.gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    def answer_question(self, question):
        retrieved_docs = self.retrieve_documents(question)
        context = " ".join(retrieved_docs)
        return self.generate_answer(context, question)

# Inicializando o sistema
rag_system = RAGSystem()


# Testando o sistema RAG
def test_rag(question):
    print("Pergunta:", question)
    print("\nResposta:", rag_system.answer_question(question))
    print("-" * 50)

# Exemplos de perguntas
perguntas = [
    "Quais sÃ£o as Ã¡reas de especializaÃ§Ã£o de Danilo?",
    "Qual Ã© a formaÃ§Ã£o acadÃªmica de Danilo?",
    "Quais tecnologias Danilo utiliza no seu trabalho?"
]

for pergunta in perguntas:
    test_rag(pergunta)

